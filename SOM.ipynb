{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xOATk3P-IDR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1Ykzc-y-GJv"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, num_weights, x, y):\n",
        "        self.weights = np.random.rand(num_weights)\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def calculate_distance(self, input_vector):\n",
        "        distance = np.sum((self.weights - input_vector) ** 2)\n",
        "        return distance\n",
        "\n",
        "    def adjust_weights(self, target, learning_rate, influence):\n",
        "        self.weights += learning_rate * influence * (target - self.weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEzsC5x2-L0C"
      },
      "outputs": [],
      "source": [
        "class SOM:\n",
        "    def __init__(self, x, y, num_iterations, size, learning_rate):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.num_iterations = num_iterations\n",
        "        self.size_of_input_vector = size\n",
        "        self.initial_learning_rate = learning_rate\n",
        "\n",
        "        self.som = []\n",
        "        for i in range(self.x):\n",
        "            for j in range(self.y):\n",
        "                self.som.append(Node(self.size_of_input_vector, i, j))\n",
        "\n",
        "        self.map_radius = max(self.x, self.y) / 2\n",
        "        self.time_constant = num_iterations / np.log(self.map_radius)\n",
        "        self.iteration_count = 0\n",
        "        self.done = False\n",
        "\n",
        "    def epoch(self, data):\n",
        "        if len(data[0]) != self.size_of_input_vector:\n",
        "            return False\n",
        "\n",
        "        if self.done:\n",
        "            return True\n",
        "\n",
        "        if self.iteration_count < self.num_iterations:\n",
        "            current_data = np.random.randint(0, len(data))\n",
        "\n",
        "            # Finding BMU\n",
        "            bmu = self.find_best_matching_node(data[current_data])\n",
        "\n",
        "            # Exponential decay of radius and learning rate\n",
        "            self.neighbourhood_radius = self.map_radius * np.exp(-self.iteration_count / self.time_constant)\n",
        "            self.learning_rate = self.initial_learning_rate * np.exp(-self.iteration_count / self.num_iterations)\n",
        "\n",
        "            # Adjusting weights\n",
        "            for node in self.som:\n",
        "                dist_to_node_sq = (node.x - bmu.x)**2 + (node.y - bmu.y)**2\n",
        "                if dist_to_node_sq < self.neighbourhood_radius ** 2:\n",
        "                    influence = np.exp(-dist_to_node_sq / (2 * (self.neighbourhood_radius ** 2)))\n",
        "                    node.adjust_weights(data[current_data], self.learning_rate, influence)\n",
        "\n",
        "            self.iteration_count += 1\n",
        "        else:\n",
        "            self.done = True\n",
        "\n",
        "        return True\n",
        "\n",
        "    def find_best_matching_node(self, vec):\n",
        "        lowest_distance = float('inf')\n",
        "        winner = None\n",
        "\n",
        "        for node in self.som:\n",
        "            dist = node.calculate_distance(vec)\n",
        "            if dist < lowest_distance:\n",
        "                lowest_distance = dist\n",
        "                winner = node\n",
        "\n",
        "        return winner\n",
        "\n",
        "    # Plotting SOM grid to show clustering\n",
        "    def plot_grid(self, data, uniquesIDs):\n",
        "      grid = np.zeros((self.x, self.y, 3))\n",
        "      self.colorMapping = []\n",
        "\n",
        "      # Assigning weights to r, g, b for more spread out cluster colors\n",
        "      red_weight, green_weight, blue_weight = 1.0, 0.9, 0.6\n",
        "      for i in range(self.x):\n",
        "          for j in range(self.y):\n",
        "              rgb = np.zeros(3)\n",
        "              weights = self.som[i * self.y + j].weights\n",
        "              for k in range(len(weights)):\n",
        "                  if k % 3 == 0:\n",
        "                      rgb[0] += weights[k] * red_weight\n",
        "                  elif k % 3 == 1:\n",
        "                      rgb[1] += weights[k] * green_weight\n",
        "                  elif k % 3 == 2:\n",
        "                      rgb[2] += weights[k] * blue_weight\n",
        "              rgb /= sum(rgb)\n",
        "\n",
        "              grid[i,j] = rgb\n",
        "              coordinates = (i, j)\n",
        "\n",
        "      # Creating mapping between colors, cities and cluster coordinates\n",
        "      xIndex = 0\n",
        "      for x in data:\n",
        "          bestClusterCoordinates = self.find_best_matching_node(x).x, self.find_best_matching_node(x).y\n",
        "          iD = uniquesIDs[xIndex]\n",
        "          self.colorMapping.append([iD, bestClusterCoordinates])\n",
        "          xIndex += 1\n",
        "\n",
        "      for lst in self.colorMapping:\n",
        "          coordinates = lst[1]\n",
        "          lst.append(grid[coordinates[0], coordinates[1]])\n",
        "\n",
        "      # Plot the grid\n",
        "      plt.imshow(grid)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    # Visulaing dataset on world map\n",
        "    def plot_map(self):\n",
        "        worldmap = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "        _, ax = plt.subplots()\n",
        "        worldmap.plot(ax=ax, facecolor='white', edgecolor='black')\n",
        "        countries = worldmap[\"name\"].tolist()\n",
        "\n",
        "        for i in self.colorMapping:\n",
        "            if i[0] in countries:\n",
        "                worldmap[worldmap.name == i[0]].plot(color=i[2], ax=ax)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VOx--fN6-PdD",
        "outputId": "b0ff6e21-6eaf-4189-98e1-7695e1ceaa6f"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/Q1_countrydata.csv\")\n",
        "\n",
        "# Replacing name of country in data with name corresponding to geopanda worldmap file for plotting\n",
        "data.replace('United States', 'United States of America', inplace = True)\n",
        "data.replace( 'Russian Federation', 'Russia',inplace = True)\n",
        "data.replace('Congo (Kinshasa)', 'Dem. Rep. Congo', inplace = True)\n",
        "data.replace('Congo (Brazzaville)', 'Congo', inplace = True)\n",
        "data.replace('Central African Republic', 'Central African Rep.', inplace = True)\n",
        "data.replace('South Sudan', 'S. Sudan', inplace = True)\n",
        "data.replace('Taiwan*', 'Taiwan', inplace = True)\n",
        "data.replace('Korea, South', 'South Korea', inplace = True)\n",
        "data.replace('Eswatini', 'eSwatini', inplace = True)\n",
        "data.replace('Equatorial Guinea', 'Eq. Guinea', inplace = True)\n",
        "aggregated_data = data.groupby('Country_Region').agg({\n",
        "    'Confirmed': 'sum',\n",
        "    'Deaths': 'sum',\n",
        "    'Recovered': 'sum'\n",
        "}).reset_index()\n",
        "normalized_data = aggregated_data.copy()\n",
        "cols_to_normalize = ['Confirmed', 'Deaths', 'Recovered']\n",
        "for col in cols_to_normalize:\n",
        "    normalized_data[col] = (aggregated_data[col] - aggregated_data[col].min()) / (aggregated_data[col].max() - aggregated_data[col].min())\n",
        "\n",
        "# Extract numeric columns and convert to NumPy array\n",
        "numeric_data = normalized_data[['Confirmed', 'Deaths', 'Recovered']].values\n",
        "\n",
        "uniquesIDs = data['Country_Region'].unique()\n",
        "\n",
        "som = SOM(10, 10, 5000, 3, 0.05)\n",
        "som.plot_grid(numeric_data, uniquesIDs)\n",
        "while not som.done:\n",
        "    som.epoch(numeric_data)\n",
        "    if som.iteration_count % 100 == 0:\n",
        "        som.plot_grid(numeric_data, uniquesIDs)\n",
        "\n",
        "som.plot_grid(numeric_data, uniquesIDs)\n",
        "som.plot_map()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvyV7f6j2Lhc"
      },
      "source": [
        "Reference: http://www.ai-junkie.com/ann/som/som4.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
